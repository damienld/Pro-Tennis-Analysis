{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d46cf817",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-1-337dc7ee27d3>, line 53)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-337dc7ee27d3>\"\u001b[1;36m, line \u001b[1;32m53\u001b[0m\n\u001b[1;33m    \"\"\"\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statistics\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error \n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def indexCoeff0ByCourt(nameCourt):\n",
    "    switcher={\n",
    "            'Clay':2,\n",
    "            'Grass':3,\n",
    "            'Hard':4,\n",
    "            'I.hard':5\n",
    "            }\n",
    "    return switcher.get(nameCourt)\n",
    "\n",
    "class TournamentStats:\n",
    "    def __init__(self,id,dateT, name, speed, court,level,location\n",
    "                 , sumActual, sumExp, nbMatches, coeff0Regression, coeffRegAceFor,coeffRegAceVs):\n",
    "        self.id=id\n",
    "        self.dateT = dateT\n",
    "        self.name = name\n",
    "        self.speed = speed\n",
    "        self.court = court\n",
    "        self.level=level\n",
    "        self.location=location\n",
    "        self.sumActual=sumActual\n",
    "        self.sumExp=sumExp\n",
    "        self.nbMatches=nbMatches\n",
    "        self.coeff0Regression=coeff0Regression\n",
    "        self.coeffRegAceFor=coeffRegAceFor\n",
    "        self.coeffRegAceVs=coeffRegAceVs\n",
    "    def __repr__(self):\n",
    "        return '{},{},{},{},{},{},{},{},{},{},{},{},{}'.format(self.name,self.id,round(self.speed,2),\n",
    "                                    self.dateT, self.court, self.level, self.location,\n",
    "                                    round(self.sumActual,2), round(self.sumExp,2), self.nbMatches,\n",
    "                                    round(self.coeff0Regression,2),round(self.coeffRegAceFor,2)\n",
    "                                    ,round(self.coeffRegAceVs,2))\n",
    "    def __cmp__(self, other):\n",
    "        if hasattr(other, 'speed'):\n",
    "            return self.speed.__cmp__(other.speed)\n",
    "    def isCourt(self, courtName):\n",
    "        return (courtName==self.court)\n",
    "    @staticmethod\n",
    "    def FilterByCourtName(list, courtName):\n",
    "        return filter(isCourt(courtName), list)\n",
    "\n",
    "def getTrnsSpeeds(listTrnIds, dataFromFile, coeffsRegression, coeff0Regression):\n",
    "\"\"\"\n",
    "Calculate the Speed of a Tournament (difference between Actual Aces and Expectes Aces)\n",
    "\"\"\"\n",
    "    listTrn=[]\n",
    "    for trnId in listTrnIds:\n",
    "        strClauseTrn = 'TrnId=='+str(trnId)+' & '\n",
    "        dfMatchesForTrn=dataFromFile.copy(deep=True)\n",
    "        dfMatchesForTrn.dropna()\n",
    "        dfMatchesForTrn = dfMatchesForTrn.replace('NaN',-1).query(strClauseTrn + 'IsValidData>0 & IsEnoughData>0 & AceRatePlayer>=0 & AvgAceRateP>0  & AvgAceRateOpp>0')\n",
    "        courtCoeff0Reg = 0\n",
    "        #print('Coeff0 for Court=' + str(courtCoeff0Reg))\n",
    "        #surface not taken into account here!! else add courtCoeff0Reg below\n",
    "        expectedAce = (courtCoeff0Reg+coeff0Regression+coeffsRegression[0]*dfMatchesForTrn['AvgAceRateP']+coeffsRegression[1]*dfMatchesForTrn['AvgAceRateOpp'])\n",
    "        expectedAce= [max(t, 0.01) for t in expectedAce]  #no negative values\n",
    "        #diff=df['AceRatePlayer']-expectedAce\n",
    "        #diff= max(diff, 0.01)\n",
    "        sumActual=sum(dfMatchesForTrn['AceRatePlayer'])\n",
    "        sumExp=sum(expectedAce)\n",
    "        #print(df.iloc[0,2]+':'+str(statistics.mean(diff)))\n",
    "        #print('Sum Ace Rates:'+str(sumActual))\n",
    "        #print('Sum Exp Rates:'+str(sumExp))\n",
    "        #print(df.iloc[0,0] + df.iloc[0,2]+'Diff:'+str(sumActual/sumExp))\n",
    "        trn=TournamentStats(dfMatchesForTrn.iloc[0,0], dfMatchesForTrn.iloc[0,1], dfMatchesForTrn.iloc[0,2]\n",
    "                            , sumActual/sumExp, dfMatchesForTrn.iloc[0,5]\n",
    "                            , dfMatchesForTrn.iloc[0,3], dfMatchesForTrn.iloc[0,4], sumActual, sumExp\n",
    "                            , len(dfMatchesForTrn), coeff0Regression, coeffsRegression[0],coeffsRegression[1])\n",
    "        #print(trn)\n",
    "        listTrn.append(trn)\n",
    "    #listTrnClay=[t for t in listTrn if (x.isCourt('Clay'))]\n",
    "    #listTrnClay=filter(lambda x: x.isCourt('Clay'), listTrn)\n",
    "    #list(listTrnClay)\n",
    "    #listTrnClay.sort(key=lambda x: x.speed, reverse=True)\n",
    "    #sorted(listTrnClay, key=lambda x: x.speed, reverse=True)\n",
    "    listTrn.sort(key=lambda x: x.name, reverse=True)\n",
    "    print(*listTrn, sep=\"\\n\")\n",
    "    #listTrNonClay=list(filter(lambda x: not x.isCourt('Clay'), listTrn))\n",
    "    #sorted(listTrNonClay, key=lambda x: x.speed, reverse=True)\n",
    "    #print(*listTrNonClay, sep=\"\\n\")\n",
    "\n",
    "def displayAllMatches(trnId, dataFromFile, coeffsRegression, coeff0Regression):\n",
    "    df=dataFromFile.copy(deep=True)\n",
    "    df.dropna()\n",
    "    df = df.replace('NaN',-1).query('TrnId=='+trnId+' & IsValidData>0 & IsEnoughData>0 & AceRatePlayer>0 & AvgAceRateP>0  & AvgAceRateOpp>0')     \n",
    "    print(df)\n",
    "    for index, row in df.iterrows():\n",
    "        print(row['Player']+' in '+ row['P1'] + '/' + row['P2'])\n",
    "        courtCoeff0Reg = 0\n",
    "        expectedAce = (courtCoeff0Reg+coeff0Regression+coeffsRegression[0]*row['AvgAceRateP']+coeffsRegression[1]*row['AvgAceRateOpp'])\n",
    "        expectedAce= [max(t, 0.01) for t in expectedAce]  #no negative values\n",
    "        print('Res=' + str(row['AceRatePlayer']) + ' - Expected=' + str(round(expectedAce,2)) + ' From ' + str(row['AvgAceRateP']) +' vs '  + str(row['AvgAceRateOpp']))\n",
    "        diff=(row['AceRatePlayer']-expectedAce)\n",
    "        #diff= min(max(diff, 0.01),1.99)\n",
    "        print(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd9942b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD DATA\n",
    "dataFromFile = pd.read_csv('./Data/ATP_all_matches.csv')    \n",
    "loops=0\n",
    "while (loops<1):\n",
    "    loops+=1\n",
    "    indexCourt = '0'#input(\"Enter Surface Id(NonClay=0,Clay=1,Hard=2,Indoor=3,Grass=4): \")\n",
    "    strClauseCourt=''\n",
    "    if (indexCourt =='1'):\n",
    "        strClauseCourt = 'CourtId==\"Clay\" & '\n",
    "    if (indexCourt =='0'):\n",
    "        strClauseCourt = 'CourtId!=\"Clay\" & '\n",
    "    if (indexCourt =='2'):\n",
    "        strClauseCourt = 'CourtId==\"Hard\" & '\n",
    "    if (indexCourt =='3'):\n",
    "        strClauseCourt = 'CourtId==\"I.hard\" & '\n",
    "    if (indexCourt =='4'):\n",
    "        strClauseCourt = 'CourtId==\"Grass\" & '\n",
    "    dfMatchesForTrn=dataFromFile.copy(deep=True)\n",
    "    dfMatchesForTrn.dropna()\n",
    "    dfMatchesForTrn = dfMatchesForTrn.replace('NaN',-1).query(strClauseCourt + 'IsValidData >0 & IsEnoughData>0 & AceRatePlayer>0 & AvgAceRateP>0  & AvgAceRateOpp>0')\n",
    "    #print(dfMatchesForTrn)\n",
    "    \n",
    "    #dfMatchesForTrn=dfMatchesForTrn[(dfMatchesForTrn['AvgAceRateP']>=0) & (dfMatchesForTrn['AvgAceRateP']<5)]\n",
    "    print('nb matches: ' + str(len(dfMatchesForTrn)))\n",
    "    X=dfMatchesForTrn[[\"AvgAceRateP\",\"AvgAceRateOpp\",\"TrnSpeed2\"]]\n",
    "    X[\"AvgAceRateP\"]=X.AvgAceRateP/100\n",
    "    X[\"AvgAceRateOpp\"]=X.AvgAceRateOpp/100\n",
    "    Y=dfMatchesForTrn.AceRatePlayer\n",
    "    #X=pd.concat[poly2d.fit_transform(X.AvgAceRateP),X.AvgAceRateOpp]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef802c3",
   "metadata": {},
   "source": [
    "# EXPLORATORY DATA ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa794f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLOT\n",
    "X.hist()\n",
    "X.describe()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e691e5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.hist()\n",
    "Y.describe()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9899f10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.pairplot(X, diag_kind=\"kde\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57036dd",
   "metadata": {},
   "source": [
    "# ANALYSIS of the couples: AvgAceRateP, AvgAceRateOpp\n",
    "WHAT is the median of AceRatePlayer for each couple of features:\n",
    "STEPS:\n",
    "- round the values so they can be groupped\n",
    "- make a pivot table indexed by AvgAceRateP,AvgAceRateOpp\n",
    "- keep only when the number of rows is a minimum (20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff021a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfMatchesForTrn2=dfMatchesForTrn[[\"AceRatePlayer\",\"AvgAceRateP\",\"AvgAceRateOpp\"]]\n",
    "#round the values\n",
    "#dfMatchesForTrn2.AceRatePlayer=dfMatchesForTrn2.AceRatePlayer.round()\n",
    "dfMatchesForTrn2.AvgAceRateP=dfMatchesForTrn2.AvgAceRateP.round()\n",
    "dfMatchesForTrn2.AvgAceRateOpp=dfMatchesForTrn2.AvgAceRateOpp.round()\n",
    "#Make a pivot table indexed by the differents couples of features\n",
    "dfpivot_aces=pd.pivot_table(dfMatchesForTrn2,index=[\"AvgAceRateP\",\"AvgAceRateOpp\"],aggfunc=[\"median\", \"count\"])\n",
    "#Keep only when the number of rows is a minimum\n",
    "dfpivot_aces=dfpivot_aces[dfpivot_aces[\"count\"][\"AceRatePlayer\"]>20]\n",
    "dfpivot_aces=dfpivot_aces.reset_index()\n",
    "dfpivot_aces[dfpivot_aces.AvgAceRateOpp==5].head(22)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94776f2",
   "metadata": {},
   "source": [
    "- We knew the average of aces is about 8.0\n",
    "\n",
    "HYPOTHESIS\n",
    "- So when AvgAceRateOpp=8 we should see that AceRatePlayer=AvgAceRateP (because the 2nd feature is at the median and shouldnt impact much because the level of the opponent is really neutral)\n",
    "\n",
    "CONLUSION\n",
    "- Indeed for most couples we've got (AvgAceRateP,8)=AceRatePlayer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126f4161",
   "metadata": {},
   "source": [
    "# PLOTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08704827",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1=[len( (dfMatchesForTrn[ (dfMatchesForTrn.AvgAceRateP.round()==round(valX)) & (dfMatchesForTrn.AceRatePlayer==round(valY))]) )/5 for valX,valY in zip(dfMatchesForTrn.AvgAceRateP,dfMatchesForTrn.AceRatePlayer)]\n",
    "s2=[len( (dfMatchesForTrn[ (dfMatchesForTrn.AvgAceRateOpp.round()==round(valX)) & (dfMatchesForTrn.AceRatePlayer==round(valY))]) )/5 for valX,valY in zip(dfMatchesForTrn.AvgAceRateOpp,dfMatchesForTrn.AceRatePlayer)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670b8cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X.AvgAceRateP.round(), Y.round(), color='blue',s=s1)\n",
    "plt.xlabel('AvgAceRateP')\n",
    "plt.ylabel('ActualAceRateP')\n",
    "plt.show()\n",
    "plt.scatter(X.AvgAceRateOpp.round(), Y.round(), color='red',s=s2)\n",
    "plt.xlabel('AvgAceRateOpp')\n",
    "plt.ylabel('ActualAceRateP')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7557c63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Y_test, y_pred, color = 'blue',s=1)\n",
    "plt.xlabel('True Values [aces]')\n",
    "plt.ylabel('Predictions [aces]')\n",
    "plt.xlim([0,15])\n",
    "plt.ylim([0,15])\n",
    "plt.plot([0,15], [0,15])\n",
    "f=X.AvgAceRateP*coeffsRegression[0]+X.AvgAceRateOpp*coeffsRegression[1]+X.TrnSpeed2*coeffsRegression[2]+lm.intercept_\n",
    "print(f.iloc[0])\n",
    "#plt.plot(X.AvgAceRateP,f, '-r')\n",
    "#plt.plot(train_x, coeffsRegression[0]*train_x + regr.intercept_[0], '-r')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b7f57a",
   "metadata": {},
   "source": [
    "# LINEAR REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51abdb21",
   "metadata": {},
   "source": [
    "EVALUATION\n",
    "we compare the actual values and predicted values to calculate the accuracy of a regression model. Evaluation metrics provide a key role in the development of a model, as it provides insight to areas that require improvement.\n",
    "\n",
    "METRICS:\n",
    "- Mean absolute error: It is the mean of the absolute value of the errors. This is the easiest of the metrics to understand since it’s just average error.\n",
    "- Mean Squared Error (MSE): Mean Squared Error (MSE) is the mean of the squared error. It’s more popular than Mean absolute error because the focus is geared more towards large errors. This is due to the squared term exponentially increasing larger errors in comparison to smaller ones.\n",
    "- Root Mean Squared Error (RMSE): R-squared is not error, but is a popular metric for accuracy of your model. It represents how close the data are to the fitted regression line. The higher the R-squared, the better the model fits your data. Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d66e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def print_metrics(Y_test, y_pred):    \n",
    "    print('Mean absolute error: %.2f' % np.mean(np.absolute(y_pred - Y_test)))\n",
    "    print('Residual sum of squares (MSE): %.2f' % np.mean((y_pred - Y_test) **2 ))\n",
    "    print('R2-score: %.2f' % r2_score(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353a8aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=dfMatchesForTrn[[\"AvgAceRateP\",\"AvgAceRateOpp\",\"TrnSpeed2\"]]\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=125)\n",
    "lm=LinearRegression()\n",
    "lm.fit(X_train, Y_train)\n",
    "print(X.iloc[0,:])\n",
    "#RESULTS\n",
    "print('RESULTS FOR COURT:' + indexCourt)\n",
    "coeffsLRegression=lm.coef_\n",
    "print(coeffsLRegression)\n",
    "#print(pd.DataFrame({'estimated coeff':coeffsRegression}, index=X_train.columns))\n",
    "coeff0LRegression= lm.intercept_\n",
    "print(coeff0LRegression)\n",
    "y_pred=lm.predict(X_test)\n",
    "print_metrics(Y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fe04d3",
   "metadata": {},
   "source": [
    "# POLYNOMIAL REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a3cf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly=dfMatchesForTrn[[\"AvgAceRateP\",\"AvgAceRateOpp\",\"TrnSpeed2\"]]\n",
    "X_poly = poly.fit_transform(X_poly)\n",
    "print(X_poly[0])\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_poly, Y, test_size=0.25, random_state=125)\n",
    "lm=LinearRegression()\n",
    "lm.fit(X_train, Y_train)\n",
    "\n",
    "#RESULTS\n",
    "print('RESULTS FOR COURT:' + indexCourt)\n",
    "coeffsRegression=lm.coef_\n",
    "print('COEFFS AceP(1) AceOpp(2) Trn(3) AceP**2(4) AceP*AceOpp(5) AceP*Trn(6) AceOpp**2(7) AceOpp*Trn(8) Trn**2(9)\\n'+ str(coeffsRegression))\n",
    "#print(pd.DataFrame({'estimated coeff':coeffsRegression}, index=X_train.columns))\n",
    "coeff0Regression= lm.intercept_\n",
    "print(coeff0Regression)\n",
    "y_pred=lm.predict(X_test)\n",
    "\n",
    "print_metrics(Y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098cfca9",
   "metadata": {},
   "source": [
    "Let's apply the regression coefficients on our table dfpivot_aces\n",
    "We'll apply TrnSpeed=1 as it s the mean value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ee1487",
   "metadata": {},
   "outputs": [],
   "source": [
    "trnSpeed=1\n",
    "dfpivot_aces[\"expAces_reg\"] = [(coeff0LRegression+ace*coeffsLRegression[0]+acevs*coeffsLRegression[1]+trnSpeed*coeffsLRegression[2]) for (ace, acevs) in zip(dfpivot_aces.AvgAceRateP, dfpivot_aces.AvgAceRateOpp)]\n",
    "dfpivot_aces[\"expAces_poly\"] = [(coeff0Regression+ace*coeffsRegression[0]+acevs*coeffsRegression[1]+trnSpeed*coeffsRegression[2]+ace**2*coeffsRegression[3]+ace*acevs*coeffsRegression[4]+ace*trnSpeed*coeffsRegression[5]+acevs**2*coeffsRegression[6]+acevs*trnSpeed*coeffsRegression[7]+trnSpeed**2*coeffsRegression[8]) for (ace, acevs) in zip(dfpivot_aces.AvgAceRateP, dfpivot_aces.AvgAceRateOpp)]\n",
    "dfpivot_aces[\"expAces_reg_gap\"] = (dfpivot_aces[\"median\"][\"AceRatePlayer\"]-dfpivot_aces[\"expAces_reg\"]).abs()\n",
    "dfpivot_aces[\"expAces_poly_gap\"] = (dfpivot_aces[\"median\"][\"AceRatePlayer\"]-dfpivot_aces[\"expAces_poly\"]).abs()\n",
    "dfpivot_aces.iloc[81:100,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f333710b",
   "metadata": {},
   "source": [
    "# Application of Decision Tree regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a06a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn import metrics\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=125)\n",
    "\n",
    "dt_regressor = DecisionTreeRegressor(random_state = 125)\n",
    "dt_regressor.fit(X_train,Y_train)\n",
    "#Predicting using test set \n",
    "y_pred = dt_regressor.predict(X_test)\n",
    "mae=metrics.mean_absolute_error(Y_test, y_pred)\n",
    "mse=metrics.mean_squared_error(Y_test, y_pred)\n",
    "# Printing the metrics\n",
    "print('Suppport Vector Regression Accuracy: ', dt_regressor.score(X_test,Y_test))\n",
    "print('R2 square:',metrics.r2_score(Y_test, y_pred))\n",
    "print('MAE: ', mae)\n",
    "print('MSE: ', mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdc5424",
   "metadata": {},
   "source": [
    "# Application of Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9382231",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf_regressor = RandomForestRegressor(n_estimators = 30 ,  random_state = 0)\n",
    "rf_regressor.fit(X_train,Y_train)\n",
    "#Predicting the SalePrices using test set \n",
    "y_pred = rf_regressor.predict(X_test)\n",
    "mae=metrics.mean_absolute_error(Y_test, y_pred)\n",
    "mse=metrics.mean_squared_error(Y_test, y_pred)\n",
    "# Printing the metrics\n",
    "print('Suppport Vector Regression Accuracy: ', rf_regressor.score(X_test,Y_test))\n",
    "print('R2 square:',metrics.r2_score(Y_test, y_pred))\n",
    "print('MAE: ', mae)\n",
    "print('MSE: ', mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0581af4f",
   "metadata": {},
   "source": [
    "# Application of Support Vector Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b532122",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "regressor= SVR(kernel='rbf')\n",
    "regressor.fit(X_train,Y_train)\n",
    "y_pred_svm=regressor.predict(X_test)\n",
    "#y_pred_svm = cross_val_predict(regressor, x, y)\n",
    "mae=metrics.mean_absolute_error(Y_test, y_pred_svm)\n",
    "mse=metrics.mean_squared_error(Y_test, y_pred_svm)\n",
    "# Printing the metrics\n",
    "print('Suppport Vector Regression Accuracy: ', regressor.score(X_test,Y_test))\n",
    "print('R2 square:',metrics.r2_score(Y_test, y_pred_svm))\n",
    "print('MAE: ', mae)\n",
    "print('MSE: ', mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed8f7b7",
   "metadata": {},
   "source": [
    "# Application of Deep Learning using Keras library\n",
    "Here is the deep learning model mentioned. A sequential model has been used. The model has been created as a function named build_model so that we can call it anytime it is required in the process. The model has two connected hidden layers with a Rectified Linear Unit (relu) function and an output layer with a linear function.\n",
    "\n",
    "The hidden layers have 12 and 8 neurons respectively with all the 8 input variables. Mean Squared Error is the loss function here as it is the most common loss function in case of regression problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12d8ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def build_model():\n",
    "  model = keras.Sequential([\n",
    "    layers.Dense(12,kernel_initializer='normal', activation='relu', input_shape=[len(train_dataset.keys())]),\n",
    "    layers.Dense(8, activation='relu'),\n",
    "    layers.Dense(1, activation='linear')\n",
    "  ])\n",
    "\n",
    "  optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
    "\n",
    "  model.compile(loss='mse',\n",
    "                optimizer=optimizer,\n",
    "                metrics=['mae', 'mse'])\n",
    "  return model\n",
    "model = build_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe6714c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d8faf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
